{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import compose\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import RandomOverSampler\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n\n* Drop features (i.e. columns) which have more than 60% NaN values\n* Convert numerical feature 'Timestamp' to hour\n* Impute NaN data in numerical features with median and categorical features with most frequent values, repectively\n* Standardize numerical features\n* Encode categorical features as an integer array\n* Undersample the majority class on the class boundary\n\nReferences:\n* https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n* https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/\n* https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html","metadata":{}},{"cell_type":"code","source":"# load training data\ndf_train = pd.read_csv('/kaggle/input/suspicious-transaction-detection/train.csv', sep = ',')\n\n# drop features which have too many NaN values\ndf_train.dropna(thresh=df_train.shape[0]*0.4,how='all',axis=1,inplace=True)\n\n# convert Timestamp feature to hour\ndf_train['Timestamp'] = pd.to_datetime(df_train['Timestamp'])\ndf_train['Timestamp'] = df_train['Timestamp'].dt.hour\n\n# split features and label\nX_train = df_train.iloc[:,2:]\ny_train = df_train.iloc[:,1]\n\n# get a list of all features\ncol = X_train.columns.values.tolist()\ndel df_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get a list of categorical features\ncategorical_features = ['Goods', 'A_0', 'A_1', 'E_0', 'E_1', 'M_0', 'M_1',\n                          'C_0', 'C_1', 'C_2', 'C_3', 'C_4', 'C_5', 'C_6',\n                          'C_7', 'C_8', 'C_23', 'C_24', 'C_25', 'C_26', 'C_27',\n                         'C_28', 'O_2', 'O_4', 'O_6', 'O_7', 'O_8', 'O_10',\n                         'O_11', 'O_12', 'O_13', 'O_14', 'O_15', 'O_16', 'O_17',\n                         'O_19', 'O_21', 'O_23', 'O_24', 'O_25', 'O_26', 'O_27',\n                         'O_28', 'O_30', 'O_31', 'O_32', 'O_35', 'O_36', 'O_39']\n\n# get a list of numerical features\nnum_features = []\ncat_features = []\nfor f in col:\n    if f not in categorical_features:\n        num_features.append(f)\n    else:\n        cat_features.append(f)\n        \n\n# impute NaN data in both numerical and categorical features\n# standardize numerical features and encode categorical features\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('cat_imputer',SimpleImputer(strategy='most_frequent')),\n    ('encoder',OrdinalEncoder(handle_unknown='ignore'))])\n\npreprocessor = compose.ColumnTransformer(transformers=[('num', numerical_transformer, num_features),\n                                                       ('cat', categorical_transformer, cat_features)])\n\nX_train_enc = preprocessor.fit_transform(X_train)\nprint(X_train_enc.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The training data is imbalanced with the fraud class being the minority class. We use TomekLinks to \n# locate the boundary between the two classes and to remove some data points from the majority class \n# close to the boundary.\n\ntl = TomekLinks(sampling_strategy='auto')\nX_train_tl, y_train_tl = tl.fit_resample(X_train_enc, y_train)\n\nprint(X_train_tl.shape, y_train_tl.shape)\n\ndel X_train_enc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load and preprocess testing data\ndf_test = pd.read_csv('/kaggle/input/suspicious-transaction-detection/test.csv', sep = ',')\ndf_test['Timestamp'] = pd.to_datetime(df_test['Timestamp'])\ndf_test['Timestamp'] = df_test['Timestamp'].dt.hour\n\nT_id = df_test.iloc[:,0]\nX_test = df_test[col]\ndel df_test\n\nX_test_enc = preprocessor.transform(X_test)\ndel X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model selection\nWe trained a Random Forest model because it is an emsemble learning method and can lower the risk of overfitting. We didn't use Naive Bayes because we were not confident to assume Gaussian distribution for some features such as time, transaction amount, etc. We didn't use neural networks because there are too many features in our dataset and it would require too much computation to train a neural network model. Random Forest model runs more efficiently on our dataset. Another benefit is that it is a tree-based method and it's easy to interpret the model. \nIn the training process, we used Pipeline to set up the steps of estimation and GridSearchCV to choose the hyperparameters (i.e. the number of trees in the forest and the maximum depth of the tree) from pre-specified hyperparameters range: n_estimators = [200,500,800], max_depth = [20,50,100]. We used 5-fold cross-validation to choose the hyperparameters that maximize roc_auc score (i.e. area under the receiver operating characteristic curve). GridSearchCV returns the best model (n_estimators=800, max_depth=50) which is then refitted on the whole dataset. Its mean validation AUC score is 0.93, which is acceptable. However, due to possible timeout or memonry leakage problem with GridSearchCV, we directly use the tuned hyperparameter (n_estimators=800, max_depth=50) below to train the final model.","metadata":{}},{"cell_type":"code","source":"'''\n# Random Forest model\nestimator_rf = Pipeline(steps = [('rf', RandomForestClassifier())])\nparam_rf = {'rf__n_estimators':[200,500,800],'rf__max_depth':[20,50,100]}  # to save code running time, here we simplified the hyperparameters range to the best model we found\n\n# do 5-fold cross-validation to find the best hyperparameters\nmodel_rf = GridSearchCV(estimator_rf,param_grid = param_rf, cv=5, scoring = 'roc_auc', verbose = 0, n_jobs=-1)\n\nprint(model_rf.best_estimator_)\nprint(model_rf.best_score_)\n'''\n\n# directly use the result of GridSearchCV with the tuned hyperparameters\nmodel_rf = RandomForestClassifier(n_estimators = 800, max_depth = 50)\n\n# train the model\nmodel_rf.fit(X_train_tl, y_train_tl)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions on the testing data\n# this returns the probabilities that the transaction is suspicious ('Target'=1)\nprob = model_rf.predict_proba(X_test_enc)\ndf2 = pd.DataFrame({'Target': prob[:,1]})\nresult = pd.concat([T_id, df2], axis=1)\n\n# save results\nresult.to_csv('/kaggle/working/submission_rf.csv', index=False, header=True)\n\nprint(result.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sort the features and Plot 10 most important features","metadata":{}},{"cell_type":"code","source":"# importances = model_rf.best_estimator_.named_steps[\"rf\"].feature_importances_\nimportances = model_rf.feature_importances_\nindices = np.argsort(importances)[::-1]\ncol = np.asarray(col)\ncol_sort = col[indices]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train_tl.shape[1]):\n    print(\"%d. feature %s (%f)\" % (f + 1, col_sort[f], importances[indices[f]]))\n\n# Plot the impurity-based feature importances of the forest\nplt.figure()\nplt.title(\"Top 10 important features\")\nplt.bar(range(10), importances[indices][:10])\nplt.xticks(range(10),col_sort[:10],rotation=30)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}